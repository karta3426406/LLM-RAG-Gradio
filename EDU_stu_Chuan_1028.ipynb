{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951bebc1-e435-4d8b-978c-405812e2aa4a",
   "metadata": {},
   "source": [
    "/* 安裝函式庫 */\n",
    "1. No Module name 'docx'\n",
    "=> conda install conda-forge::python-docx\n",
    "\n",
    "2. ImportError: cannot import name 'cached_download' from 'huggingface_hub' (C:\\Users\\Fgadmin\\anaconda3\\envs\\LLM-RAG\\lib\\site-packages\\huggingface_hub\\__init__.py)\n",
    "Downgrading from 0.26.1 to 0.20.2 worked for me.\n",
    "=> conda install -c conda-forge huggingface_hub=0.20.2\n",
    "\n",
    "/* 使用 Google 大型語言模型 API */\n",
    "https://aistudio.google.com/app/apikey?hl=zh-tw => Get API key\n",
    "API 和服務 => 已啟用的 API 和服務 => 啟用 Generative Language API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30125c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fgadmin\\anaconda3\\envs\\LLM-RAG\\lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.40.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response JSON: {'candidates': [{'content': {'parts': [{'text': '## 康芮颱風路徑南修 恐增為強颱 氣象署：最快明天下午發海警\\n\\n**台北訊**  中央氣象署預報中心科長林伯東今（29）日上午表示，康芮颱風目前在鵝鑾鼻東南方1050公里海面朝西前進，未來朝西北方向移動靠近台灣，預計周四及周五影響最大，11月1日2日逐漸遠離。康芮颱風目前持續增強中，有可能增為中度颱風，甚至不排除增為強颱。\\n\\n林伯東指出，上午8點最新路徑預測與凌晨兩點相較再往南修，且颱風速度稍稍快一點，不排除登陸，但路徑仍有變化，仍有不確定性。如果從東半邊通過，中南部下雨不明顯，如果比較偏南通過，南部甚至中部下雨就要留意一下。\\n\\n根據最新預測，康芮颱風速度稍稍快一點，明天下午到晚上可能發海警，周三上半天發陸警。\\n\\n林伯東提醒，康芮颱風暴風半徑大，全台都會受影響，未來一周台灣天氣都會受其影響。請民眾密切注意氣象資訊，做好防颱準備。 \\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0, 'safetyRatings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}], 'usageMetadata': {'promptTokenCount': 330, 'candidatesTokenCount': 286, 'totalTokenCount': 616}, 'modelVersion': 'gemini-1.5-flash-001'}\n",
      "Response JSON: {'candidates': [{'content': {'parts': [{'text': '## 康芮颱風最新預測：\\n\\n* 路徑再往南修，速度稍快，不排除登陸台灣。\\n* 影響時間：周四、周五最大，11月1、2日逐漸遠離。\\n* 增強中，可能增為中度或強颱。\\n* 全台都會受影響，明天下午到晚上可能發海警，周三上半天發陸警。\\n* 路徑仍有變化，登陸地點不確定，影響程度需視颱風路徑而定。\\n* 中南部降雨需留意。\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'index': 0, 'safetyRatings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE'}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE'}]}], 'usageMetadata': {'promptTokenCount': 330, 'candidatesTokenCount': 128, 'totalTokenCount': 458}, 'modelVersion': 'gemini-1.5-flash-001'}\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import docx\n",
    "import requests\n",
    "import torch  # Import torch for topk function\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the model for semantic search\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # Lightweight model for document search\n",
    "\n",
    "# Function to read and extract text from the uploaded DOCX file\n",
    "def read_docx(file):\n",
    "    doc = docx.Document(file.name)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return full_text  # Return list of paragraphs\n",
    "\n",
    "# Function to perform document retrieval using semantic search\n",
    "def retrieve_relevant_content(task, paragraphs):\n",
    "    # Task-based query to retrieve relevant sections\n",
    "    if task == \"立法院擬答\":\n",
    "        query = \"向立法院起草正式的模擬回應，回覆應明確指出要點並提供專業的語氣\"\n",
    "    elif task == \"新聞稿\":\n",
    "        query = \"寫出一個具備教育專業語氣的新聞稿\"\n",
    "    elif task == \"總結內容\":\n",
    "        query = \"總結出重點摘要\"\n",
    "    elif task == \"生成教育相關內容\":\n",
    "        query = \"挑選出與教育內容相關的段落，並給予更深入的詳細解說\"\n",
    "    else:\n",
    "        return \"Invalid task selected.\", []\n",
    "\n",
    "    # Perform semantic search to get the most relevant paragraphs\n",
    "    paragraph_embeddings = model.encode(paragraphs, convert_to_tensor=True)\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    scores = util.pytorch_cos_sim(query_embedding, paragraph_embeddings)[0]\n",
    "    top_k = min(5, len(paragraphs))  # Retrieve top 5 relevant paragraphs\n",
    "    top_results = torch.topk(scores, k=top_k)  # Use torch.topk to get the top results\n",
    "\n",
    "    # Collect relevant paragraphs\n",
    "    relevant_paragraphs = [paragraphs[idx] for idx in top_results.indices]\n",
    "    return \" \".join(relevant_paragraphs)  # Join paragraphs into one string\n",
    "\n",
    "# Function to interact with the Gemini LLM API\n",
    "def generate_response(task, file):\n",
    "    # Read file content as a list of paragraphs\n",
    "    paragraphs = read_docx(file)\n",
    "    \n",
    "    # Retrieve relevant content\n",
    "    relevant_content = retrieve_relevant_content(task, paragraphs)\n",
    "    \n",
    "    # Define the API endpoint and your API key\n",
    "    api_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent\"\n",
    "    api_key = \" 你的 API key \"  # Replace with your actual API key\n",
    "\n",
    "    # Define the headers and payload\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Construct the API prompt based on relevant content\n",
    "    prompt = f\"{task} 根據以下內容: {relevant_content}\"\n",
    "\n",
    "    # Construct the API payload\n",
    "    payload = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [\n",
    "                    {\"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(f\"{api_url}?key={api_key}\", json=payload, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Print the entire response for debugging\n",
    "        print(\"Response JSON:\", response.json())\n",
    "        \n",
    "        # Extract the generated text from the response structure\n",
    "        response_json = response.json()\n",
    "        if \"candidates\" in response_json and len(response_json[\"candidates\"]) > 0:\n",
    "            generated_text = response_json[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "            return generated_text\n",
    "        else:\n",
    "            return \"No generated text found\"\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\"\n",
    "\n",
    "# Gradio Interface\n",
    "dropdown_choices = [\"立法院擬答\", \"新聞稿\", \"總結內容\", \"生成教育相關內容\"]\n",
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"## Gemini-Integrated Chatbot with RAG\")\n",
    "    task = gr.Dropdown(choices=dropdown_choices, label=\"Select Task\")\n",
    "    file = gr.File(label=\"Upload DOCX File\", file_types=[\".docx\"])\n",
    "    output = gr.Textbox(label=\"Generated Output\")\n",
    "    \n",
    "    submit_btn = gr.Button(\"Generate\")\n",
    "    submit_btn.click(generate_response, inputs=[task, file], outputs=output)\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebce58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
